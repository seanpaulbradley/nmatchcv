{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seanpaulbradley/nmatchcv/blob/main/Copy_of_THINGSVISION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAV0bWMzg2lJ"
      },
      "source": [
        "# THINGSvision\n",
        "This is the PyTorch version, you can find a TensorFlow example [here](https://colab.research.google.com/github/ViCCo-Group/THINGSvision/blob/master/doc/tensorflow.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4r4x5pjg2lL"
      },
      "source": [
        "## Settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ICWd-3iA671"
      },
      "source": [
        "### Install thingsvision and dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0nVMt-M_KX_"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade thingsvision\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1NNA4gyg2lM"
      },
      "outputs": [],
      "source": [
        "!pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yq-bNySyBGO-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from thingsvision import get_extractor\n",
        "from thingsvision.utils.storing import save_features\n",
        "from thingsvision.utils.data import ImageDataset, DataLoader\n",
        "from thingsvision.core.extraction import center_features\n",
        "\n",
        "from google.colab import drive\n",
        "from typing import Any, Dict, List, Optional, Union"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQhfZWwQg2lN"
      },
      "source": [
        "### Image and feature directories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHSuNkaIAZw2"
      },
      "source": [
        "Specify both `path/to/images` (input directory) and `path/to/features` (output directory) on your Google Drive.\n",
        "The image directory is expected to contain images that are saved similarly to `/dog/img_1.png` or `/cat/img_1.jpg`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RsALIkKg2lN"
      },
      "outputs": [],
      "source": [
        "image_dir = 'path/to/images'  # path/to/images in GDrive\n",
        "output_dir = 'path/to/features' # path/to/output  in GDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF0R7sFu-7gI"
      },
      "source": [
        "Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8nY_u1p-1F6"
      },
      "outputs": [],
      "source": [
        "mounted_dir = '/thingsvision'\n",
        "drive.mount(mounted_dir, force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-Q-0Ztfg2lO"
      },
      "outputs": [],
      "source": [
        "full_image_path = os.path.join(mounted_dir, 'MyDrive', image_dir)\n",
        "full_output_path = os.path.join(mounted_dir, 'MyDrive', output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MJxmeVig2lO"
      },
      "source": [
        "### Helper functions to extract features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkYIhI_P_Z6t"
      },
      "outputs": [],
      "source": [
        "def extract_features(\n",
        "    extractor: Any,\n",
        "    module_name: str,\n",
        "    image_path: str,\n",
        "    out_path: str,\n",
        "    batch_size: int,\n",
        "    flatten_activations: bool,\n",
        "    apply_center_crop: bool,\n",
        "    class_names: Optional[List[str]]=None,\n",
        "    file_names: Optional[List[str]]=None,\n",
        ") -> Union[np.ndarray, torch.Tensor]:\n",
        "    \"\"\"Extract features for a single layer.\"\"\"\n",
        "    dataset = ImageDataset(\n",
        "        root=image_path,\n",
        "        out_path=out_path,\n",
        "        backend=extractor.get_backend(),\n",
        "        transforms=extractor.get_transformations(apply_center_crop=apply_center_crop, resize_dim=256, crop_dim=224),\n",
        "        class_names=class_names,\n",
        "        file_names=file_names,\n",
        "    )\n",
        "    batches = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        backend=extractor.get_backend(),\n",
        "        )\n",
        "    features = extractor.extract_features(\n",
        "        batches=batches,\n",
        "        module_name=module_name,\n",
        "        flatten_acts=flatten_activations,\n",
        "        output_type=\"ndarray\", # or \"tensor\" (only applicable to PyTorch models)\n",
        "    )\n",
        "    return features\n",
        "\n",
        "\n",
        "def extract_all_layers(\n",
        "    model_name: str,\n",
        "    extractor: Any,\n",
        "    image_path: str,\n",
        "    out_path: str,\n",
        "    batch_size: int,\n",
        "    flatten_activations: bool,\n",
        "    apply_center_crop: bool,\n",
        "    layer: Any=nn.Linear,\n",
        "    file_format: str = \"npy\",\n",
        "    class_names: Optional[List[str]]=None,\n",
        "    file_names: Optional[List[str]]=None,\n",
        ") -> Dict[str, Union[np.ndarray, torch.Tensor]]:\n",
        "    \"\"\"Extract features for all selected layers and save them to disk.\"\"\"\n",
        "    features_per_layer = {}\n",
        "    for l, (module_name, module) in enumerate(extractor.model.named_modules(), start=1):\n",
        "        if isinstance(module, layer):\n",
        "            # extract features for layer \"module_name\"\n",
        "            features = extract_features(\n",
        "                extractor=extractor,\n",
        "                module_name=module_name,\n",
        "                image_path=image_path,\n",
        "                out_path=out_path,\n",
        "                batch_size=batch_size,\n",
        "                flatten_activations=flatten_activations,\n",
        "                apply_center_crop=apply_center_crop,\n",
        "                class_names=class_names,\n",
        "                file_names=file_names,\n",
        "            )\n",
        "            # replace with e.g., [f'conv_{l:02d}'] or [f'fc_{l:02d}']\n",
        "            features_per_layer[f'layer_{l:02d}'] = features\n",
        "            # save features to disk\n",
        "            save_features(features, out_path=f'{out_path}/features_{model_name}_{module_name}', file_format=file_format)\n",
        "    return features_per_layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUakPkZrg2lP"
      },
      "source": [
        "### Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvJFUBiJ_j0W"
      },
      "outputs": [],
      "source": [
        "pretrained = True # use pretrained model weights\n",
        "model_path = None # if pretrained = False (i.e., randomly initialized weights) set path to model weights\n",
        "batch_size = 32 # use a power of two (this can be any size, depending on the number of images for which you aim to extract features)\n",
        "apply_center_crop = True # center crop images (set to False, if you don't want to center-crop images)\n",
        "flatten_activations = True # whether or not features (e.g., of Conv layers) should be flattened\n",
        "class_names = None  # optional list of class names for class dataset\n",
        "file_names = None # optional list of file names according to which features should be sorted\n",
        "file_format = \"npy\" # format with which to save features to disk (can be set to \"mat\", \"txt\", \"npy\", \"hdf5\")\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySiLlCmjg2lP"
      },
      "source": [
        "Select `model` and `layer` for which you want to extract image features. If you want to extract features from a `torchvision` model, use the model naming defined [here](https://pytorch.org/vision/stable/models.html) (e.g., `vgg16` if you want to use VGG-16). If you are uncertain about the naming and enumeration of the layers, use `extractor.show_model()` to see how specific layers are called. This command will show the architecture of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SThuaouCg2lP"
      },
      "source": [
        "### Example 1: VGG-16 with batch norm (pretrained on ImageNet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBMvmbEDg2lP"
      },
      "source": [
        "Note that it is crucial to set a model's `source`. VGG16 implementations exist in different libraries and therefore (pretrained) weights can be downloaded from different sources. One such source is `torchvision` from which we will download VGG16."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54H332Z1g2lP"
      },
      "outputs": [],
      "source": [
        "# load model\n",
        "model_name = 'vgg16_bn'\n",
        "# specify model source\n",
        "# we use torchvision here (https://pytorch.org/vision/stable/models.html)\n",
        "source = 'torchvision'\n",
        "# initialize the extractor\n",
        "extractor = get_extractor(\n",
        "    model_name=model_name,\n",
        "    pretrained=pretrained,\n",
        "    model_path=model_path,\n",
        "    device=device,\n",
        "    source=source\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpQojy2Og2lP"
      },
      "outputs": [],
      "source": [
        "## select layer\n",
        "\n",
        "# NOTE: uncomment the line below, if you are uncertain about layer naming in PyTorch\n",
        "# extractor.show_model()\n",
        "module_name = 'features.23'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS6p_U8dg2lQ"
      },
      "source": [
        "#### Feature extraction single layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "tCGsjNJHg2lQ"
      },
      "outputs": [],
      "source": [
        "# extract features for a single layer (e.g., penultimate)\n",
        "features = extract_features(\n",
        "    extractor=extractor,\n",
        "    module_name=module_name,\n",
        "    image_path=full_image_path,\n",
        "    out_path=full_output_path,\n",
        "    batch_size=batch_size,\n",
        "    flatten_activations=flatten_activations,\n",
        "    apply_center_crop=apply_center_crop,\n",
        "    class_names=class_names,\n",
        "    file_names=file_names,\n",
        ")\n",
        "\n",
        "# apply centering (not necessary, but may be desirable, depending on the analysis)\n",
        "features = center_features(features)\n",
        "\n",
        "# save features to disk\n",
        "save_features(features, out_path=f'{full_output_path}/features_{model_name}_{module_name}', file_format=file_format)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4HJ8lRGg2lQ"
      },
      "source": [
        "#### Feature extraction all convolutional or fully-connected layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAweLnKNg2lQ"
      },
      "outputs": [],
      "source": [
        "# extract features for all convolutional layers (i.e., Conv2d) and save them to disk\n",
        "layer = nn.Conv2d\n",
        "features_conv_layers = extract_all_layers(\n",
        "    model_name=model_name,\n",
        "    extractor=extractor,\n",
        "    image_path=full_image_path,\n",
        "    out_path=full_output_path,\n",
        "    batch_size=batch_size,\n",
        "    flatten_activations=flatten_activations,\n",
        "    apply_center_crop=apply_center_crop,\n",
        "    layer=layer,\n",
        "    file_format=file_format,\n",
        "    class_names=class_names,\n",
        "    file_names=file_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSD5CT3tg2lQ"
      },
      "outputs": [],
      "source": [
        "# extract features for all fully-connected layers (i.e., Linear) and save them to disk\n",
        "layer = nn.Linear\n",
        "features_fc_layers = extract_all_layers(\n",
        "    model_name=model_name,\n",
        "    extractor=extractor,\n",
        "    image_path=full_image_path,\n",
        "    out_path=full_output_path,\n",
        "    batch_size=batch_size,\n",
        "    flatten_activations=flatten_activations,\n",
        "    apply_center_crop=apply_center_crop,\n",
        "    layer=layer,\n",
        "    file_format=file_format,\n",
        "    class_names=class_names,\n",
        "    file_names=file_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rdmg9wjAg2lQ"
      },
      "source": [
        "### Example 2: VGG-16 (pretrained on Ecoset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "PbD4vIDNg2lQ"
      },
      "outputs": [],
      "source": [
        "## load model\n",
        "model_name = 'VGG16_ecoset'\n",
        "# specifiy model source\n",
        "# the model's source here is custom\n",
        "source = 'custom'\n",
        "extractor = get_extractor(\n",
        "    model_name=model_name,\n",
        "    pretrained=pretrained,\n",
        "    model_path=model_path,\n",
        "    device=device,\n",
        "    source=source,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tzI1zO9g2lQ"
      },
      "outputs": [],
      "source": [
        "## select layer\n",
        "\n",
        "# NOTE: uncomment the line below, if you are uncertain about layer naming in PyTorch and look at architecture\n",
        "# extractor.show_model()\n",
        "module_name = 'features.23'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDDQ8_Isg2lQ"
      },
      "source": [
        "#### Feature extraction single layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBM1mlw6g2lR"
      },
      "outputs": [],
      "source": [
        "# extract features for a single layer (e.g., penultimate)\n",
        "vgg_ecoset_features = extract_features(\n",
        "    extractor=extractor,\n",
        "    module_name=module_name,\n",
        "    image_path=full_image_path,\n",
        "    out_path=full_output_path,\n",
        "    batch_size=batch_size,\n",
        "    flatten_activations=flatten_activations,\n",
        "    apply_center_crop=apply_center_crop,\n",
        "    class_names=class_names,\n",
        "    file_names=file_names,\n",
        ")\n",
        "\n",
        "# apply centering (not necessary, but may be desirable, depending on the analysis)\n",
        "vgg_ecoset_features = center_features(vgg_ecoset_features)\n",
        "\n",
        "# save features to disk\n",
        "save_features(vgg_ecoset_features, out_path=f'{full_output_path}/features_{model_name}_{module_name}', file_format=file_format)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKgWT-K7g2lR"
      },
      "source": [
        "#### Feature extraction all convolutional or fully-connected layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3fmfszzg2lR"
      },
      "outputs": [],
      "source": [
        "# extract features for all convolutional layers (i.e., Conv2d) and save them to disk\n",
        "layer = nn.Conv2d\n",
        "features_conv_layers = extract_all_layers(\n",
        "    model_name=model_name,\n",
        "    extractor=extractor,\n",
        "    image_path=full_image_path,\n",
        "    out_path=full_output_path,\n",
        "    batch_size=batch_size,\n",
        "    flatten_activations=flatten_activations,\n",
        "    apply_center_crop=apply_center_crop,\n",
        "    layer=layer,\n",
        "    file_format=file_format,\n",
        "    class_names=class_names,\n",
        "    file_names=file_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOLGCqgog2lR"
      },
      "outputs": [],
      "source": [
        "# extract features for all fully-connected layers (i.e., Linear) and save them to disk\n",
        "layer = nn.Linear\n",
        "features_fc_layers = extract_all_layers(\n",
        "    model_name=model_name,\n",
        "    extractor=extractor,\n",
        "    image_path=full_image_path,\n",
        "    out_path=full_output_path,\n",
        "    batch_size=batch_size,\n",
        "    flatten_activations=flatten_activations,\n",
        "    apply_center_crop=apply_center_crop,\n",
        "    layer=layer,\n",
        "    file_format=file_format,\n",
        "    class_names=class_names,\n",
        "    file_names=file_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohtX88PTg2lR"
      },
      "source": [
        "### Example 3: CLIP (multimodal pretraining)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCgUh5phg2lR"
      },
      "outputs": [],
      "source": [
        "# load model\n",
        "model_name = 'clip'\n",
        "variant = 'ViT-B/32'\n",
        "# specifiy model source\n",
        "# the model's source here is custom\n",
        "source = 'custom'\n",
        "extractor = get_extractor(\n",
        "    model_name=model_name,\n",
        "    pretrained=pretrained,\n",
        "    model_path=model_path,\n",
        "    device=device,\n",
        "    source=source,\n",
        "    model_parameters={'variant': variant}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvEusG78g2lR"
      },
      "outputs": [],
      "source": [
        "## select layer\n",
        "\n",
        "# NOTE: uncomment the line below, if you are uncertain about layer naming of CLIP\n",
        "# extractor.show_model()\n",
        "module_name = 'visual' # image features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHN9bmXLg2lR"
      },
      "source": [
        "#### Feature extraction single layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DVikzwNg2lR"
      },
      "outputs": [],
      "source": [
        "# extract features\n",
        "clip_features = extract_features(\n",
        "    extractor=extractor,\n",
        "    module_name=module_name,\n",
        "    image_path=full_image_path,\n",
        "    out_path=full_output_path,\n",
        "    batch_size=batch_size,\n",
        "    flatten_activations=flatten_activations,\n",
        "    apply_center_crop=apply_center_crop,\n",
        "    class_names=class_names,\n",
        "    file_names=file_names,\n",
        ")\n",
        "\n",
        "# apply centering (not necessary, but may be desirable, depending on the analysis)\n",
        "clip_features = center_features(clip_features)\n",
        "\n",
        "# save features to disk\n",
        "save_features(clip_features, out_path=f'{full_output_path}/features_{model_name}_{module_name}', file_format=file_format)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Skm6RLymg2lV"
      },
      "source": [
        "### Representational Similarity Analysis (RSA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZz8FVDEg2lV"
      },
      "outputs": [],
      "source": [
        "from thingsvision.core.rsa import compute_rdm, plot_rdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKXPwvlFg2lV"
      },
      "outputs": [],
      "source": [
        "# compute representational dissimilarity matrix (RDM) for CLIP features\n",
        "rdm = compute_rdm(clip_features, method='correlation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2zw1YXBg2lV"
      },
      "outputs": [],
      "source": [
        "# plot rdm\n",
        "plot_rdm(\n",
        "        full_output_path,\n",
        "        clip_features,\n",
        "        method='correlation',\n",
        "        format='.png', # '.jpg'\n",
        "        colormap='cividis',\n",
        "        show_plot=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjHJVYJWg2lV"
      },
      "source": [
        "### Centered Kernel Alignment (CKA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbb1IhCKg2lW"
      },
      "outputs": [],
      "source": [
        "from thingsvision.core.cka import CKA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbtSLr5ng2lW"
      },
      "outputs": [],
      "source": [
        "assert vgg_ecoset_features.shape[0] == clip_features.shape[0]\n",
        "m = clip_features.shape[0]\n",
        "cka = CKA(m=m, kernel='linear')\n",
        "rho = cka.compare(X=vgg_ecoset_features, Y=clip_features)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}